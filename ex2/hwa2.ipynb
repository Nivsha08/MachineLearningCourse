{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Decision Trees\n",
    "\n",
    "In this assignment you will implement a Decision Tree algorithm as learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write vectorized code whenever possible.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in the provided `hw2.py` python module only. All the logic you write is imported and used in this jupyter notebook.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports detected in `hw2.py` will earn you the grade of 0, even if you only used them for testing.\n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install. Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes the `hw2.py` file and this notebook. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "9. You are allowed to include additional functions.\n",
    "10. Submission: zip only the completed jupyter notebook and the python file `hw2.py`. Do not include the data or any directories. Name the file `ID1_ID2.zip` and submit only one copy of the assignment.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Practice OOP in python.\n",
    "2. Implement two impurity measures: Gini and Entropy.\n",
    "3. Implement a decision tree from scratch.\n",
    "4. Prune the tree to achieve better results.\n",
    "5. Visualize your results and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "63bcec20cb406a2169b49168f173c760",
     "grade": false,
     "grade_id": "cell-ed9fe7b1026e33cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hw2 import * # this imports all functions from hw2.\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup - OOP in python\n",
    "\n",
    "Our decision tree will be implemented using a dedicated python class. Python classes are very similar to classes in Java.\n",
    "\n",
    "\n",
    "You can use the following [site](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/) to learn about classes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Node at 0x1a202277f0>, <__main__.Node at 0x1a202d8a20>]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We will use the breast cancer dataset that is available as a part of sklearn - a popular machine learning and data science library in python. In this example, our dataset will be a single matrix with the **labels on the last column**. Notice that you are not allowed to use additional functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e2620b671ce98b8a856d59ce9f95f71",
     "grade": false,
     "grade_id": "cell-d79cb4542926ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (426, 31)\n",
      "Testing dataset shape:  (143, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y = True)\n",
    "X = np.column_stack([X,y]) # the last column holds the labels\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize class example dataset\n",
    "class_dataset = np.array([[0,0,1,0,0],\n",
    "                   [0,0,1,1,0],\n",
    "                   [1,0,1,0,1],\n",
    "                   [2,1,1,0,1],\n",
    "                   [2,2,0,0,1],\n",
    "                   [2,2,0,1,0],\n",
    "                   [1,2,0,1,1],\n",
    "                   [0,1,1,0,0],\n",
    "                   [0,2,0,0,1],\n",
    "                   [2,1,0,0,1],\n",
    "                   [0,1,0,1,1],\n",
    "                   [1,1,1,1,1],\n",
    "                   [1,0,0,0,1],\n",
    "                   [2,1,1,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measures\n",
    "\n",
    "Implement the functions `calc_gini` (5 points) and `calc_entropy` (5 points) in the python file `hw2.py`. You are encouraged to test your implementation using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9402859586706311\n",
      "entropy V\n",
      "0.4591836734693877\n",
      "gini V\n"
     ]
    }
   ],
   "source": [
    "# data from class enropy example\n",
    "# entropy test \n",
    "entropy = calc_entropy(class_dataset)\n",
    "print(entropy)\n",
    "print('entropy V' if np.isclose(entropy, 0.940, rtol=1e-01) else 'entropy X')\n",
    "\n",
    "# gini test\n",
    "gini = calc_gini(class_dataset)\n",
    "print(gini)\n",
    "print('gini V' if np.isclose(gini, 0.450, rtol=1e-01) else 'gini X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree\n",
    "\n",
    "Use a Python class to construct the decision tree (look at the `DecisionNode` class in the python file `hw2.py`. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes and you are free to use them as you see fit. We recommend that every node will hold the feature and value used for the split and its children.\n",
    "2. Your code should support both Gini and Entropy as impurity measures. \n",
    "3. The provided data includes continuous data. In this exercise, create at most a single split for each node of the tree. The threshold you need to use for this exercise are the average of each consecutive pair of values. For example, assume some features contains the following values: [1,2,3,4,5]. You should use the following thresholds [1.5, 2.5, 3.5, 4.5]. \n",
    "4. When constructing the tree, test all possible thresholds for each feature. The stopping criteria is a pure tree.\n",
    "\n",
    "Complete the class `DecisionNode` in the python file `hw2.py`. The structure of this class is entirely up to you. Complete the function `build_tree` in the python file `hw2.py`. This function should get the training dataset and the impurity as inputs, initiate a root for the decision tree and construct the tree according to the procedure you learned in class. (30 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 2.5, 3.5, 4.5]\n"
     ]
    }
   ],
   "source": [
    "# threshold function test\n",
    "test_values = [1,2,3,4,5]\n",
    "test_values = np.column_stack([test_values, np.zeros(len(test_values))])\n",
    "test_thresholds = build_thresholds_for_attribute_values(test_values, 0)\n",
    "print(test_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test settings definitions on X_train data\n",
    "test_attribute_index = 8\n",
    "test_threshold = 0.23\n",
    "test_impurity_function = calc_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1185,\n",
       " 0.12090000000000001,\n",
       " 0.12175,\n",
       " 0.12625,\n",
       " 0.13065,\n",
       " 0.1325,\n",
       " 0.13455,\n",
       " 0.13495000000000001,\n",
       " 0.13515,\n",
       " 0.13565,\n",
       " 0.13625,\n",
       " 0.13690000000000002,\n",
       " 0.13774999999999998,\n",
       " 0.1384,\n",
       " 0.1387,\n",
       " 0.13965,\n",
       " 0.1407,\n",
       " 0.14100000000000001,\n",
       " 0.1416,\n",
       " 0.14215,\n",
       " 0.14250000000000002,\n",
       " 0.14300000000000002,\n",
       " 0.1437,\n",
       " 0.14455,\n",
       " 0.14515,\n",
       " 0.14565,\n",
       " 0.14615,\n",
       " 0.14650000000000002,\n",
       " 0.14665,\n",
       " 0.14695,\n",
       " 0.14725,\n",
       " 0.14795,\n",
       " 0.14865,\n",
       " 0.1488,\n",
       " 0.1492,\n",
       " 0.14955000000000002,\n",
       " 0.14975,\n",
       " 0.15025,\n",
       " 0.1507,\n",
       " 0.15095,\n",
       " 0.15125,\n",
       " 0.15145,\n",
       " 0.15155000000000002,\n",
       " 0.15185,\n",
       " 0.1527,\n",
       " 0.15339999999999998,\n",
       " 0.15360000000000001,\n",
       " 0.15375,\n",
       " 0.15385,\n",
       " 0.15410000000000001,\n",
       " 0.15435,\n",
       " 0.1545,\n",
       " 0.1548,\n",
       " 0.1552,\n",
       " 0.15545,\n",
       " 0.1558,\n",
       " 0.15625,\n",
       " 0.15645,\n",
       " 0.15655,\n",
       " 0.15685,\n",
       " 0.1572,\n",
       " 0.15765,\n",
       " 0.15810000000000002,\n",
       " 0.15825,\n",
       " 0.15835,\n",
       " 0.15845,\n",
       " 0.15855,\n",
       " 0.1587,\n",
       " 0.15885,\n",
       " 0.15895,\n",
       " 0.15915,\n",
       " 0.15935,\n",
       " 0.15949999999999998,\n",
       " 0.15985,\n",
       " 0.16015000000000001,\n",
       " 0.16045,\n",
       " 0.161,\n",
       " 0.1614,\n",
       " 0.16155,\n",
       " 0.1617,\n",
       " 0.16185,\n",
       " 0.16194999999999998,\n",
       " 0.16205,\n",
       " 0.1624,\n",
       " 0.16275,\n",
       " 0.1629,\n",
       " 0.16305,\n",
       " 0.16315000000000002,\n",
       " 0.16325,\n",
       " 0.16335,\n",
       " 0.16344999999999998,\n",
       " 0.16365000000000002,\n",
       " 0.16394999999999998,\n",
       " 0.16435,\n",
       " 0.16465000000000002,\n",
       " 0.16475,\n",
       " 0.16485,\n",
       " 0.16494999999999999,\n",
       " 0.16510000000000002,\n",
       " 0.16525,\n",
       " 0.16549999999999998,\n",
       " 0.1659,\n",
       " 0.16615,\n",
       " 0.16625,\n",
       " 0.16635,\n",
       " 0.16654999999999998,\n",
       " 0.16675,\n",
       " 0.16685,\n",
       " 0.16699999999999998,\n",
       " 0.16720000000000002,\n",
       " 0.16755,\n",
       " 0.1679,\n",
       " 0.16815000000000002,\n",
       " 0.1684,\n",
       " 0.16875,\n",
       " 0.1691,\n",
       " 0.1694,\n",
       " 0.16965,\n",
       " 0.1699,\n",
       " 0.17020000000000002,\n",
       " 0.17035,\n",
       " 0.17054999999999998,\n",
       " 0.1708,\n",
       " 0.17099999999999999,\n",
       " 0.17120000000000002,\n",
       " 0.17135,\n",
       " 0.17154999999999998,\n",
       " 0.17175,\n",
       " 0.1719,\n",
       " 0.17204999999999998,\n",
       " 0.17215,\n",
       " 0.1723,\n",
       " 0.17254999999999998,\n",
       " 0.17295,\n",
       " 0.17325000000000002,\n",
       " 0.17335,\n",
       " 0.17345,\n",
       " 0.17359999999999998,\n",
       " 0.1738,\n",
       " 0.174,\n",
       " 0.17415,\n",
       " 0.17425000000000002,\n",
       " 0.17435,\n",
       " 0.1748,\n",
       " 0.17565,\n",
       " 0.1764,\n",
       " 0.1768,\n",
       " 0.17695,\n",
       " 0.17704999999999999,\n",
       " 0.17720000000000002,\n",
       " 0.17745,\n",
       " 0.17770000000000002,\n",
       " 0.17785,\n",
       " 0.17795,\n",
       " 0.17804999999999999,\n",
       " 0.17815,\n",
       " 0.17830000000000001,\n",
       " 0.17859999999999998,\n",
       " 0.17905,\n",
       " 0.17935,\n",
       " 0.17955,\n",
       " 0.17975,\n",
       " 0.17985,\n",
       " 0.18,\n",
       " 0.18015,\n",
       " 0.1804,\n",
       " 0.18065,\n",
       " 0.18080000000000002,\n",
       " 0.18095,\n",
       " 0.18109999999999998,\n",
       " 0.18125,\n",
       " 0.18135,\n",
       " 0.18145,\n",
       " 0.18155,\n",
       " 0.18175000000000002,\n",
       " 0.18195,\n",
       " 0.18209999999999998,\n",
       " 0.18225,\n",
       " 0.18235,\n",
       " 0.18259999999999998,\n",
       " 0.18285,\n",
       " 0.18295,\n",
       " 0.18309999999999998,\n",
       " 0.18325,\n",
       " 0.18335,\n",
       " 0.1837,\n",
       " 0.18409999999999999,\n",
       " 0.18430000000000002,\n",
       " 0.18445,\n",
       " 0.18455,\n",
       " 0.18464999999999998,\n",
       " 0.18475,\n",
       " 0.185,\n",
       " 0.18525,\n",
       " 0.18535000000000001,\n",
       " 0.18545,\n",
       " 0.18555,\n",
       " 0.18575,\n",
       " 0.18595,\n",
       " 0.18605,\n",
       " 0.1864,\n",
       " 0.18675,\n",
       " 0.18685000000000002,\n",
       " 0.18705,\n",
       " 0.18730000000000002,\n",
       " 0.18745,\n",
       " 0.18755,\n",
       " 0.18775,\n",
       " 0.18795,\n",
       " 0.18805,\n",
       " 0.1883,\n",
       " 0.18855,\n",
       " 0.1888,\n",
       " 0.18914999999999998,\n",
       " 0.1894,\n",
       " 0.18955,\n",
       " 0.18964999999999999,\n",
       " 0.18985000000000002,\n",
       " 0.1901,\n",
       " 0.19025,\n",
       " 0.1905,\n",
       " 0.19075,\n",
       " 0.19085,\n",
       " 0.19105,\n",
       " 0.19145,\n",
       " 0.19185000000000002,\n",
       " 0.1921,\n",
       " 0.19235000000000002,\n",
       " 0.19255,\n",
       " 0.19265,\n",
       " 0.19275,\n",
       " 0.19285,\n",
       " 0.19295,\n",
       " 0.19305,\n",
       " 0.1933,\n",
       " 0.19355,\n",
       " 0.19365,\n",
       " 0.19385000000000002,\n",
       " 0.1941,\n",
       " 0.19425,\n",
       " 0.19435,\n",
       " 0.1945,\n",
       " 0.19469999999999998,\n",
       " 0.19485,\n",
       " 0.1951,\n",
       " 0.19535,\n",
       " 0.1955,\n",
       " 0.19565,\n",
       " 0.1958,\n",
       " 0.19605,\n",
       " 0.19640000000000002,\n",
       " 0.19665,\n",
       " 0.19690000000000002,\n",
       " 0.1972,\n",
       " 0.19745000000000001,\n",
       " 0.1977,\n",
       " 0.19815,\n",
       " 0.1987,\n",
       " 0.19895000000000002,\n",
       " 0.1991,\n",
       " 0.19924999999999998,\n",
       " 0.19940000000000002,\n",
       " 0.19965,\n",
       " 0.19995000000000002,\n",
       " 0.2002,\n",
       " 0.2006,\n",
       " 0.20095000000000002,\n",
       " 0.20140000000000002,\n",
       " 0.20185,\n",
       " 0.2022,\n",
       " 0.20255,\n",
       " 0.20265,\n",
       " 0.20285,\n",
       " 0.20305,\n",
       " 0.20329999999999998,\n",
       " 0.20355,\n",
       " 0.20365,\n",
       " 0.2039,\n",
       " 0.20475,\n",
       " 0.20555,\n",
       " 0.2059,\n",
       " 0.20650000000000002,\n",
       " 0.2072,\n",
       " 0.2077,\n",
       " 0.20805,\n",
       " 0.20834999999999998,\n",
       " 0.20855,\n",
       " 0.20865,\n",
       " 0.20905,\n",
       " 0.20945,\n",
       " 0.21005000000000001,\n",
       " 0.21085,\n",
       " 0.2112,\n",
       " 0.21145,\n",
       " 0.2118,\n",
       " 0.21215,\n",
       " 0.21234999999999998,\n",
       " 0.21255000000000002,\n",
       " 0.21275,\n",
       " 0.21284999999999998,\n",
       " 0.21300000000000002,\n",
       " 0.21315,\n",
       " 0.2142,\n",
       " 0.21545,\n",
       " 0.21595,\n",
       " 0.21625,\n",
       " 0.21644999999999998,\n",
       " 0.21705,\n",
       " 0.21815,\n",
       " 0.2192,\n",
       " 0.21965,\n",
       " 0.21995,\n",
       " 0.22035,\n",
       " 0.22110000000000002,\n",
       " 0.22195,\n",
       " 0.22285,\n",
       " 0.22365000000000002,\n",
       " 0.22435,\n",
       " 0.22499999999999998,\n",
       " 0.22515000000000002,\n",
       " 0.22635,\n",
       " 0.2288,\n",
       " 0.23020000000000002,\n",
       " 0.23065000000000002,\n",
       " 0.232,\n",
       " 0.23355,\n",
       " 0.23454999999999998,\n",
       " 0.23609999999999998,\n",
       " 0.23735,\n",
       " 0.23765,\n",
       " 0.23865,\n",
       " 0.23959999999999998,\n",
       " 0.23975000000000002,\n",
       " 0.24005,\n",
       " 0.2411,\n",
       " 0.2439,\n",
       " 0.249,\n",
       " 0.25295,\n",
       " 0.2539,\n",
       " 0.2548,\n",
       " 0.25625,\n",
       " 0.2612,\n",
       " 0.26665,\n",
       " 0.27105,\n",
       " 0.28245,\n",
       " 0.2973]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thresholds function test on X_train data\n",
    "build_thresholds_for_attribute_values(X_train, test_attribute_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9546904191180343"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted average test on X_train data\n",
    "calc_weighted_average_by_attribute(X_train, test_attribute_index, test_threshold, test_impurity_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 26)\n",
      "(367, 33)\n"
     ]
    }
   ],
   "source": [
    "# data split test on X_train data\n",
    "test_group_a, test_group_b, group_a_size, group_b_size = split_data(X_train, test_attribute_index, test_threshold)\n",
    "print(\"(%d, %d)\" % (group_a_size, group_b_size))\n",
    "seconds_split_attribute_index = 5\n",
    "second_split_test_threshold = 0.17\n",
    "test_group_a, test_group_b, group_a_size, group_b_size = split_data(test_group_a, seconds_split_attribute_index, second_split_test_threshold)\n",
    "print(\"(%d, %d)\" % (group_a_size, group_b_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test attribute column removal\n",
    "test_group_a = remove_attribute_column(test_group_a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15183550136234159, 2, 0.5)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test find best information gain on class example dataset\n",
    "find_best_information_gain_params(class_dataset, calc_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python support passing a function as arguments to another function.\n",
    "tree_gini = build_tree(data=X_train, impurity=calc_gini) \n",
    "tree_entropy = build_tree(data=X_train, impurity=calc_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree evaluation\n",
    "\n",
    "Complete the functions `predict` and `calc_accuracy` in the python file `hw2.py`. You are allowed to implement this functionality as a class method.\n",
    "\n",
    "After building both trees using the training set (using Gini and Entropy as impurity measures), you should calculate the accuracy on the test set and print the measure that gave you the best test accuracy. For the rest of the exercise, use that impurity measure. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# test predict function on X_test data\n",
    "test_instance = X_test[5, :]\n",
    "prediction = predict(tree_entropy, test_instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Accuracy: 79.02097902097903\n",
      "Entropy Accuracy: 93.00699300699301\n"
     ]
    }
   ],
   "source": [
    "# test calc_accuracy on X_test data\n",
    "gini_accuracy = calc_accuracy(tree_gini, X_test)\n",
    "entropy_accuracy = calc_accuracy(tree_entropy, X_test)\n",
    "print(\"Gini Accuracy:\", gini_accuracy)\n",
    "print(\"Entropy Accuracy:\", entropy_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi square pre-pruning\n",
    "\n",
    "Consider the following p-value cut-off values: [1 (no pruning), 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]. For each value, construct a tree and prune it according to the cut-off value. Next, calculate the training and testing accuracy. On a single plot, draw the training and testing accuracy as a function of the p-value. What p-value gives you the best results? Does the results support the theory you learned in class regarding Chi square pruning? Explain. (20 points)\n",
    "\n",
    "**Note**: You need to change the `DecisionNode` to support Chi square pruning. Make sure the `chi_value=1` corresponds to no pruning. The values you need from the Chi square table are available in the python file `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "V\n"
     ]
    }
   ],
   "source": [
    "# test chi value computation based on class example\n",
    "chi_test_data = np.array([[0,1,1],\n",
    "                          [0,1,1],\n",
    "                          [1,1,1],\n",
    "                          [1,1,1],\n",
    "                          [1,1,1],\n",
    "                          [0,0,0]])\n",
    "group_a = chi_test_data[[5], :]\n",
    "group_b = chi_test_data[[0,1,2,3,4], :]\n",
    "chi_value = chi_square_split_test(1, 0.5, group_a, group_b, 5, 1)\n",
    "print(chi_value)\n",
    "print(\"V\" if np.isclose(chi_value, 6, rtol=0) else \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 93.00699300699301, 0.01: 20.97902097902098, 0.005: 20.97902097902098, 0.001: 20.97902097902098, 0.0005: 20.97902097902098, 0.0001: 20.97902097902098, 1e-05: 20.97902097902098}\n"
     ]
    }
   ],
   "source": [
    "training = X_train\n",
    "testing  = X_test\n",
    "\n",
    "accuracy_dict = {}\n",
    "for chi_value in [1, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]:\n",
    "    accuracy_dict[chi_value] = calc_tree_accuracy_by_p_value(training, testing, calc_entropy, chi_value)\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your visualization here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning\n",
    "\n",
    "Construct a decision tree without Chi square pruning. For each leaf in the tree, calculate the test accuracy of the tree assuming no split occurred on the parent of that leaf and find the best such parent (in the sense that not splitting on that parent results in the best testing accuracy among possible parents). Make that parent into a leaf and repeat this process until you are left with just the root. On a single plot, draw the training and testing accuracy as a function of the number of internal nodes in the tree. Explain the results: what would happen to the training and testing accuracies when you remove nodes from the tree? Can you suggest a different approach to achieve better results? (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the tree\n",
    "\n",
    "Complete the function `print_tree` in the python file `hw2.py` and print the tree using the chosen impurity measure and no pruning. Your code should like something like this (10 points):\n",
    "```\n",
    "[X0 <= 1],\n",
    "  [X1 <= 2]\n",
    "    [X2 <= 3], \n",
    "       leaf: [{1.0: 10}]\n",
    "       leaf: [{0.0: 10}]\n",
    "    [X4 <= 5], \n",
    "       leaf: [{1.0: 5}]\n",
    "       leaf: [{0.0: 10}]\n",
    "   leaf: [{1.0: 50}]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Tree:\n",
      "[A27 <= 0.142350]\n",
      "   leaf: [{0: 22}]\n",
      "leaf: [{1: 249}]\n",
      "   [A13 <= 21.925000]\n",
      "      leaf: [{0: 6}]\n",
      "leaf: [{1: 8}]\n",
      "      [A4 <= 0.079285]\n",
      "         leaf: [{1: 1}]\n",
      "         leaf: [{0: 140}]\n",
      "------------------------------------\n",
      "Entropy Tree:\n",
      "[A27 <= 0.142350]\n",
      "   leaf: [{0: 22}]\n",
      "leaf: [{1: 249}]\n",
      "   [A13 <= 21.925000]\n",
      "      leaf: [{0: 6}]\n",
      "leaf: [{1: 8}]\n",
      "      [A4 <= 0.079285]\n",
      "         leaf: [{1: 1}]\n",
      "         leaf: [{0: 140}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini Tree:\")\n",
    "print_tree(tree_gini)\n",
    "print(\"------------------------------------\")\n",
    "print(\"Entropy Tree:\")\n",
    "print_tree(tree_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
